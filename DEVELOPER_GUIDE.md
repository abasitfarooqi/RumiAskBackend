# ğŸ§  Ask Rumi Backend - Complete Developer Guide

## ğŸš€ **QUICK START - Your Backend is Ready!**

Your Ask Rumi backend is **running successfully** on `http://127.0.0.1:8001` with:
- âœ… **2 Tiny Models Available**: `gemma3:270m` (0.29GB) and `qwen3:0.6b` (0.52GB)
- âœ… **Apple Metal (MPS) Detection**: Optimized for your Mac M4
- âœ… **Rumi Wisdom Database**: 15+ curated quotes ready for RAG
- âœ… **All API Endpoints Working**: Chat, Models, Providers, System

---

## ğŸ¯ **MODEL SWITCHING GUIDE**

### **Available Tiny Models (Perfect for Mac M4)**

| Model | Size | Speed | Best For |
|-------|------|-------|----------|
| `gemma3:270m` | 0.29GB | âš¡ Ultra Fast | Quick responses, testing |
| `qwen3:0.6b` | 0.52GB | âš¡ Very Fast | Balanced quality/speed |
| `llama3.2:3b` | 2.0GB | ğŸš€ Fast | Better quality responses |
| `tinyllama:1.1b` | 0.64GB | âš¡ Ultra Fast | Maximum speed |

### **Switch Models via API**

```bash
# Use Gemma 3 (fastest)
curl -X POST "http://127.0.0.1:8001/api/chat/send" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is wisdom?",
    "model": "gemma3:270m",
    "temperature": 0.7
  }'

# Use Qwen 3 (balanced)
curl -X POST "http://127.0.0.1:8001/api/chat/send" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is wisdom?",
    "model": "qwen3:0.6b",
    "temperature": 0.7
  }'
```

### **Download More Tiny Models**

```bash
# Download TinyLlama (ultra-fast)
ollama pull tinyllama:1.1b

# Download Llama 3.2 3B (balanced)
ollama pull llama3.2:3b

# Download Phi-3 Mini (Microsoft)
ollama pull phi3-mini
```

---

## ğŸ”§ **DEVELOPER WORKFLOW**

### **1. Start Development Server**

```bash
# Activate virtual environment
source venv/bin/activate

# Start server with auto-reload
python -m uvicorn main:app --reload --host 127.0.0.1 --port 8001
```

### **2. Test Your Models**

```bash
# Test model inference
curl -X POST "http://127.0.0.1:8001/api/models/run" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3:270m",
    "prompt": "Hello, how are you?",
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Test Rumi-style responses
curl -X POST "http://127.0.0.1:8001/api/chat/ask-rumi" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is love?",
    "model": "gemma3:270m"
  }'
```

### **3. Monitor System**

```bash
# Check system info
curl http://127.0.0.1:8001/api/system/info

# Check device status (should show MPS on Mac M4)
curl http://127.0.0.1:8001/api/system/device

# Check available models
curl http://127.0.0.1:8001/api/models/available
```

---

## ğŸ—ï¸ **ARCHITECTURE BREAKDOWN**

### **Core Modules Explained**

```
core/
â”œâ”€â”€ config.py          # ğŸ”§ Configuration & GPU detection
â”œâ”€â”€ gpu_manager.py     # ğŸ–¥ï¸  Apple Metal/CUDA/CPU switching
â”œâ”€â”€ model_manager.py   # ğŸ“¦ Model registry & downloads
â”œâ”€â”€ local_runner.py    # ğŸš€ Ollama inference engine
â””â”€â”€ queue_manager.py   # âš¡ Async task management
```

### **API Routes Explained**

```
routes/
â”œâ”€â”€ chat.py            # ğŸ’¬ Chat endpoints & Rumi responses
â”œâ”€â”€ models.py          # ğŸ¤– Model management & inference
â”œâ”€â”€ providers.py        # ğŸ”Œ Ollama/HuggingFace/OpenAI
â””â”€â”€ system.py          # ğŸ“Š System info & device management
```

### **Data Structure**

```
data/
â”œâ”€â”€ rumi_quotes.json   # ğŸ“š 15+ Rumi quotes with categories
â”œâ”€â”€ model_registry.json # ğŸ“‹ Model metadata & status
â””â”€â”€ providers.config.json # âš™ï¸ Provider configurations
```

---

## ğŸ¨ **KEY FEATURES IMPLEMENTED**

### **1. Smart GPU Detection**
- **Mac M4**: Automatically uses Apple Metal (MPS)
- **NVIDIA**: Falls back to CUDA
- **CPU**: Universal fallback
- **Runtime Switching**: Change devices via API

### **2. Model Management**
- **Registry System**: Track all models and their status
- **Auto-Download**: Queue model downloads
- **Status Tracking**: Monitor download progress
- **Provider Support**: Ollama, HuggingFace, OpenAI

### **3. Async Architecture**
- **Queue Manager**: Handle concurrent requests
- **Background Tasks**: Non-blocking operations
- **Streaming Support**: Real-time responses
- **Error Handling**: Robust error management

### **4. Rumi Integration**
- **Wisdom Database**: Curated quotes with categories
- **RAG Ready**: Prepared for semantic search
- **Rumi-Style Responses**: Enhanced prompts for wisdom
- **Conversation Memory**: Multi-turn conversations

---

## ğŸ” **API ENDPOINTS REFERENCE**

### **Chat Endpoints**
```bash
POST /api/chat/send              # Send message
POST /api/chat/stream            # Stream response
POST /api/chat/ask-rumi         # Rumi-style responses
GET  /api/chat/conversations     # List conversations
GET  /api/chat/conversations/{id} # Get conversation
```

### **Model Endpoints**
```bash
GET  /api/models/                # List all models
GET  /api/models/available        # List available models
POST /api/models/run              # Run inference
POST /api/models/run/stream       # Stream inference
POST /api/models/download         # Download model
GET  /api/models/{name}           # Get model info
POST /api/models/{name}/test       # Test model
```

### **System Endpoints**
```bash
GET  /api/system/info             # System information
GET  /api/system/device           # Device status
POST /api/system/device           # Switch device
GET  /api/system/memory           # Memory usage
GET  /api/system/health           # Health check
GET  /api/system/stats            # System statistics
```

### **Provider Endpoints**
```bash
GET  /api/providers/              # List providers
GET  /api/providers/{name}        # Get provider info
POST /api/providers/{name}/test    # Test provider
POST /api/providers/{name}/enable  # Enable provider
```

---

## ğŸš€ **NEXT DEVELOPMENT STEPS**

### **Phase 2: RAG & Semantic Search**
```python
# Add to services/
â”œâ”€â”€ embedding_service.py    # Generate embeddings
â”œâ”€â”€ rag_service.py          # RAG with Rumi database
â””â”€â”€ search_service.py       # Semantic search
```

### **Phase 3: Voice Integration**
```python
# Add Whisper support
â”œâ”€â”€ whisper_service.py      # Speech-to-text
â”œâ”€â”€ tts_service.py          # Text-to-speech
â””â”€â”€ audio_routes.py         # Audio endpoints
```

### **Phase 4: MicroRumiLLM**
```python
# Fine-tuning pipeline
â”œâ”€â”€ training_service.py     # Model fine-tuning
â”œâ”€â”€ dataset_service.py      # Rumi dataset management
â””â”€â”€ evaluation_service.py   # Model evaluation
```

---

## ğŸ› ï¸ **DEVELOPMENT COMMANDS**

### **Model Management**
```bash
# List available models
ollama list

# Download new model
ollama pull <model-name>

# Test model locally
ollama run <model-name> "Hello, how are you?"

# Remove model
ollama rm <model-name>
```

### **Server Management**
```bash
# Start server
python -m uvicorn main:app --reload --port 8001

# Check server status
curl http://127.0.0.1:8001/

# View API docs
open http://127.0.0.1:8001/docs
```

### **Testing Commands**
```bash
# Test system health
curl http://127.0.0.1:8001/api/system/health

# Test model inference
curl -X POST "http://127.0.0.1:8001/api/models/run" \
  -H "Content-Type: application/json" \
  -d '{"model": "gemma3:270m", "prompt": "Test"}'

# Test Rumi chat
curl -X POST "http://127.0.0.1:8001/api/chat/ask-rumi" \
  -H "Content-Type: application/json" \
  -d '{"message": "What is wisdom?", "model": "gemma3:270m"}'
```

---

## ğŸ“Š **PERFORMANCE ON MAC M4**

### **Model Performance**
- **Gemma 3 270M**: ~0.5-1s response time
- **Qwen 3 0.6B**: ~0.8-1.5s response time
- **Memory Usage**: 2-4GB RAM
- **GPU Acceleration**: Apple Metal (MPS) active

### **System Resources**
- **CPU**: 10 cores detected
- **Memory**: 16GB total, ~4GB available
- **Storage**: 460GB total, 358GB available
- **Device**: Apple Metal Performance Shaders active

---

## ğŸ¯ **QUICK TESTING CHECKLIST**

- [ ] âœ… Server running on port 8001
- [ ] âœ… Gemma 3 model responding
- [ ] âœ… Rumi chat endpoint working
- [ ] âœ… System info showing MPS device
- [ ] âœ… API documentation accessible
- [ ] âœ… Model registry updated
- [ ] âœ… All endpoints responding

---

## ğŸ”— **USEFUL LINKS**

- **API Documentation**: http://127.0.0.1:8001/docs
- **System Info**: http://127.0.0.1:8001/api/system/info
- **Available Models**: http://127.0.0.1:8001/api/models/available
- **Health Check**: http://127.0.0.1:8001/api/system/health

---

## ğŸ‰ **YOU'RE READY TO BUILD!**

Your Ask Rumi backend is **fully functional** with:
- âœ… **2 Working Models**: Gemma 3 & Qwen 3
- âœ… **Apple Metal Optimization**: Perfect for Mac M4
- âœ… **Complete API**: All endpoints working
- âœ… **Rumi Database**: Ready for RAG integration
- âœ… **Async Architecture**: Scalable and robust

**Next**: Start building your React Native frontend or add RAG capabilities! ğŸš€
